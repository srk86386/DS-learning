{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f7dd214",
   "metadata": {
    "tags": []
   },
   "source": [
    "# PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ff147b-a4dc-4700-ab17-4596eb4a888e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0974612",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "\n",
    "PySpark is a Python library that provides an interface for Apache Spark, which is a distributed computing framework for processing large datasets. PySpark allows you to write Spark applications using Python, which is a popular language for data analysis and machine learning.\n",
    "Steps to learn:\n",
    "> Understand Spark concepts: It's important to have a good understanding of Spark concepts such as RDDs (Resilient Distributed Datasets), transformations, and actions. You can find information on these topics in the Apache Spark documentation.\n",
    "\n",
    ">Start with basic PySpark operations: Once you have a good understanding of Spark concepts, you can start working with PySpark. Start with basic PySpark operations such as creating RDDs, performing transformations, and running actions.\n",
    "\n",
    ">Practice with PySpark examples: There are many PySpark examples available online that you can use to practice and learn PySpark. You can find PySpark examples in the official Apache Spark documentation, on GitHub, and on other websites.\n",
    "\n",
    "### Data types in pyspark\n",
    "><b>RDD(Resilient Distributed Datasets)</b> - RDDs are the core abstraction in Spark, and are the most low-level data structure available. RDDs are an immutable distributed collection of objects that can be processed in parallel across a cluster. RDDs are resilient to failures and can be rebuilt on demand, which makes them ideal for processing large datasets. However, RDDs have some limitations, such as lack of optimizations and slower performance than DataFrames or Datasets.\n",
    "\n",
    "><b>Dataframe</b>: DataFrames are a more high-level abstraction than RDDs, and provide a more efficient and optimized way to work with structured data. DataFrames are similar to tables in a relational database, where data is organized into rows and columns. DataFrames are immutable distributed collections of data, with named columns and optimized for querying and filtering operations. DataFrames are based on RDDs but with added schema information, which allows Spark to perform more efficient optimizations.\n",
    "\n",
    "><b>Dataset</b>: Datasets are a newer abstraction in Spark and are a hybrid between RDDs and DataFrames. Datasets are distributed collections of data with strongly typed objects, and are optimized for type-safe processing. Datasets allow Spark to perform compile-time type checking, which can help catch errors early in the development cycle. Datasets provide the flexibility and scalability of RDDs, while also providing the performance benefits of DataFrames.\n",
    "\n",
    "### Spark Concepts:\n",
    "#### Spark\n",
    ">Spark is a distributed computing framework designed for processing large datasets. It is based on the concept of Resilient Distributed Datasets (RDDs), which are fault-tolerant, immutable data structures that can be processed in parallel across multiple nodes in a cluster.\n",
    "\n",
    ">Spark operates on a master-slave architecture, where there is a central coordinator (master) that distributes tasks to multiple workers (slaves) in a cluster. The master node is responsible for managing the cluster resources, while the worker nodes are responsible for executing the tasks assigned to them.\n",
    "\n",
    ">Spark provides a high-level API in several programming languages, including Python (PySpark), Java, Scala, and R. It supports a wide range of data processing operations, such as filtering, aggregating, sorting, and joining, which can be performed on RDDs or on more optimized data structures like DataFrames and Datasets.\n",
    "\n",
    ">One of the key features of Spark is its ability to perform in-memory processing, which allows it to achieve very high processing speeds compared to traditional disk-based processing systems. Spark also provides a number of optimization techniques, such as caching, partitioning, and pipelining, which further improve its performance.\n",
    "\n",
    ">Spark is used in a wide range of applications, including data processing, machine learning, and stream processing. It is particularly well-suited for big data processing tasks, as it can scale to handle datasets that are too large to fit in a single machine's memory.\n",
    "\n",
    ">Overall, Spark is a powerful distributed computing framework that allows for efficient processing of large datasets in a distributed and fault-tolerant manner.\n",
    "\n",
    "#### Typed Object\n",
    ">a strongly typed object refers to an object whose data type is known at compile-time. In other words, the data type of a strongly typed object is known at the time the code is written, rather than being inferred at runtime.\n",
    "\n",
    "#### Type-safe\n",
    ">Type-safe processing, on the other hand, refers to a programming approach where type errors are caught at compile-time rather than at runtime. This means that if a piece of code tries to perform an operation on an object of the wrong type, the code will not compile and an error will be raised, rather than allowing the code to run and potentially causing errors at runtime.\n",
    "\n",
    "#### Spark Session\n",
    "In PySpark, a SparkSession is the entry point to a Spark application and provides a way to interact with Spark functionality. It is the primary way to create and manipulate PySpark DataFrames and other distributed data structures.<br>\n",
    "The SparkSession object is a combination of the previously used SparkContext, SQLContext, and HiveContext, which have been merged into a single object in PySpark 2.0 and later versions. The SparkSession provides a unified entry point for Spark programming, allowing you to easily configure and access Spark functionality through a single API.<br>\n",
    "The SparkSession provides several methods for creating PySpark DataFrames, such as reading data from various sources like CSV, JSON, Parquet, and Hive tables. It also provides access to Spark's machine learning (ML) library and Spark Streaming API.<br>\n",
    "Once you have created a SparkSession, you can use it to create PySpark DataFrames, access Spark's ML library, or perform other Spark operations.\n",
    "\n",
    "#### UDF (User-Defined Function) : \n",
    "is a feature in PySpark that allows you to define your own custom functions to operate on DataFrame columns. You can use UDFs to apply any Python function to a PySpark DataFrame column.\n",
    "To use a UDF in PySpark, you need to follow these steps:\n",
    "\n",
    ">Define a Python function that takes one or more arguments and returns a value.\n",
    "\n",
    ">Register the Python function as a UDF using the udf() function from the pyspark.sql.functions module. You need to specify the input and output types of the UDF.\n",
    "\n",
    ">Use the registered UDF to apply the Python function to a PySpark DataFrame column using the withColumn() method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f19fdd5",
   "metadata": {},
   "source": [
    "## Prcticle\n",
    "Pyspark dataframe from json file: To create a PySpark DataFrame from a JSON file, you can use the spark.read.json() method\n",
    "<code>\n",
    "        from pyspark.sql import SparkSession\n",
    "        # create a SparkSession\n",
    "        spark = SparkSession.builder.appName(\"JSON to DataFrame\").getOrCreate()\n",
    "        # read the JSON file into a DataFrame\n",
    "        df = spark.read.json(\"path/to/json/file.json\")\n",
    "        # display the DataFrame\n",
    "        df.show()\n",
    "</code>\n",
    "\n",
    "Pyspark RDD from json file: To create a PySpark RDD from a JSON file, you can use the sc.textFile() method to read the JSON file as a text file, and then use the json.loads() method to parse each line of the text file into a Python dictionary.\n",
    "<code>    \n",
    "    import json\n",
    "    from pyspark import SparkContext\n",
    "    # create a SparkContext\n",
    "    sc = SparkContext(\"local\", \"JSON to RDD\")\n",
    "    # read the JSON file as a text file\n",
    "    text_rdd = sc.textFile(\"path/to/json/file.json\")\n",
    "    # parse each line of the text file into a Python dictionary\n",
    "    json_rdd = text_rdd.map(lambda line: json.loads(line))\n",
    "    # display the RDD\n",
    "    json_rdd.collect()\n",
    "</code>\n",
    "\n",
    "Pyspark Dataset from json file:\n",
    "<code>\n",
    "    from pyspark.sql import SparkSession\n",
    "    # create a SparkSession\n",
    "    spark = SparkSession.builder.appName(\"JSON to Dataset\").getOrCreate()\n",
    "    # read the JSON file into a DataFrame\n",
    "    df = spark.read.json(\"path/to/json/file.json\")\n",
    "    # convert the DataFrame to a Dataset\n",
    "    ds = df.as(\"mydataset\")\n",
    "    # display the Dataset\n",
    "    ds.show()\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdf2a27-d4b8-4931-8355-b91bc7a324cb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### selecting data from pyspark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8f37afc-2ae9-434a-8a87-df9d039ba452",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/01 13:54:40 WARN Utils: Your hostname, confabsol7-Lenovo-ideapad-330-15IKB resolves to a loopback address: 127.0.1.1; using 192.168.1.4 instead (on interface wlp3s0)\n",
      "23/04/01 13:54:40 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/01 13:54:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   name|age|\n",
      "+-------+---+\n",
      "|  Rahul| 29|\n",
      "|Aashish| 28|\n",
      "|  Aashu| 30|\n",
      "|Sheetal| 22|\n",
      "+-------+---+\n",
      "\n",
      "None\n",
      "[Row(name='Rahul'), Row(name='Aashu')]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "# create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"JSON to DataFrame\").getOrCreate()\n",
    "\n",
    "df = spark.createDataFrame([('Rahul', 29), (\"Aashish\", 28), (\"Aashu\", 30), ('Sheetal',22)], ['name', 'age'])\n",
    "print(df.show())\n",
    "\n",
    "rows = df.select('name').take(3)  # Select the first three rows of the DataFrame\n",
    "selected_rows = [rows[0], rows[2]]  # Select the first and third rows\n",
    "print(selected_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a851d30-7c5a-4483-bdab-0704c7ba647b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   name|age|\n",
      "+-------+---+\n",
      "|  Rahul| 29|\n",
      "|Aashish| 28|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sliced_df = df.limit(2)\n",
    "sliced_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e26465f-8458-4aa6-b45e-63685d5072d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|age|\n",
      "+---+\n",
      "| 29|\n",
      "| 28|\n",
      "| 30|\n",
      "| 22|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sliced_df = df.select(df.columns[1:4]) #selecting 2nd, 3rd and 4th column\n",
    "sliced_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f8946a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Creating calculated columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cd7a4e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "|  a|  b|  c|\n",
      "+---+---+---+\n",
      "|  1|  2|  2|\n",
      "|  3|  4| 12|\n",
      "|  5|  6| 30|\n",
      "+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# create a DataFrame\n",
    "df = spark.createDataFrame([(1, 2), (3, 4), (5, 6)], ['a', 'b'])\n",
    "\n",
    "# add a calculated column 'c' by multiplying 'a' and 'b'\n",
    "df = df.withColumn('c', col('a') * col('b'))\n",
    "\n",
    "# show the resulting DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e018528b-c03f-4e81-97fe-f34c624ec331",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Applying user defined function using UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d15682d-5dba-4c93-bfe2-fd53295affae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+----------+\n",
      "|  a|  b|a_plus_one|\n",
      "+---+---+----------+\n",
      "|  1|  2|         2|\n",
      "|  3|  4|         4|\n",
      "|  5|  6|         6|\n",
      "+---+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# create a DataFrame\n",
    "df = spark.createDataFrame([(1, 2), (3, 4), (5, 6)], ['a', 'b'])\n",
    "\n",
    "# define a Python function to apply to the column\n",
    "def add_one(x):\n",
    "    return x + 1\n",
    "\n",
    "# create a UDF (User-Defined Function)\n",
    "add_one_udf = udf(add_one, IntegerType())\n",
    "\n",
    "# apply the UDF to the 'a' column\n",
    "df = df.withColumn('a_plus_one', add_one_udf('a'))\n",
    "\n",
    "# show the resulting DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "826a8bd0-1773-44ee-9b06-e5a6dbe58e0f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'a_plus_one']\n",
      "3\n",
      "[Row(a=1, b=2, a_plus_one=2), Row(a=3, b=4, a_plus_one=4), Row(a=5, b=6, a_plus_one=6)]\n",
      "[Row(a=1, b=2, a_plus_one=2), Row(a=3, b=4, a_plus_one=4), Row(a=5, b=6, a_plus_one=6)]\n",
      "[Row(a=1), Row(a=3), Row(a=5)]\n",
      "[Row(a=1, b=2), Row(a=3, b=4), Row(a=5, b=6)]\n",
      "[Row(a=1, b=2), Row(a=3, b=4), Row(a=5, b=6)]\n"
     ]
    }
   ],
   "source": [
    "# show columns in pyspark dataframe\n",
    "print(df.columns)\n",
    "\n",
    "# to show rows count in dataframe\n",
    "print(df.count())\n",
    "\n",
    "# show top 5 rows using head\n",
    "print(df.head(5))\n",
    "\n",
    "# show bottom 5 records\n",
    "print(df.tail(5))\n",
    "\n",
    "# select specific columns\n",
    "print(df[[df.a]].head(5))\n",
    "\n",
    "# select specific columns\n",
    "print(df.select('a', 'b').head(5))\n",
    "\n",
    "# select specific columns\n",
    "print(df.select(['a', 'b']).head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df501960-0c9e-41f6-bcfe-b4493fc25a30",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Applying and using filter and where"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7385766a-cee9-4e05-b169-0722782c4cf3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(a=3, b=4, a_plus_one=4), Row(a=5, b=6, a_plus_one=6)]\n",
      "[Row(a=3, b=4, a_plus_one=4), Row(a=5, b=6, a_plus_one=6)]\n",
      "[Row(a=3, b=4, a_plus_one=4), Row(a=5, b=6, a_plus_one=6)]\n",
      "[Row(a=3, b=4, a_plus_one=4), Row(a=5, b=6, a_plus_one=6)]\n",
      "[Row(a=3, b=4, a_plus_one=4), Row(a=5, b=6, a_plus_one=6)]\n",
      "[Row(a=3, b=4, a_plus_one=4), Row(a=5, b=6, a_plus_one=6)]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "# using filter \n",
    "print(df.filter('a > 2').head(5))\n",
    "print(df.filter(df.a > 2).head(5))\n",
    "print(df.filter(col('a') >= 2).head(5))\n",
    "\n",
    "# using where \n",
    "print(df.where('a > 2').head(5))\n",
    "print(df.where(df.a > 2).head(5))\n",
    "print(df.where(col('a') >= 2).head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26e7acf0-5569-4388-8e5f-a4e89a87b378",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(a=5, b=6, a_plus_one=6)]\n",
      "[Row(a=5, b=6, a_plus_one=6)]\n",
      "[Row(a=5, b=6, a_plus_one=6)]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "# using filter \n",
    "print(df.filter('a > 2 and b>4').head(5))\n",
    "print(df.filter((df.a > 2) & (df.b > 4)).head(5))\n",
    "print(df.filter((col('a') >= 2) & (col('b') > 4)).head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17458584-860f-4fda-b903-6a2aeba958d5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Iterating through pyspark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06e6a2e0-4ace-4de1-b394-d6084973ac9e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Row(a=1, b=2, a_plus_one=2)\n",
      "Row(a=5, b=6, a_plus_one=6)\n",
      "Row(a=3, b=4, a_plus_one=4)\n"
     ]
    }
   ],
   "source": [
    "def print_row(row):\n",
    "    print(row)\n",
    "\n",
    "df.foreach(print_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db9a3621-c679-41a6-902f-989a18f967af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(a=1, b=2, a_plus_one=2)\n",
      "Row(a=3, b=4, a_plus_one=4)\n",
      "Row(a=5, b=6, a_plus_one=6)\n"
     ]
    }
   ],
   "source": [
    "rows = df.collect()\n",
    "for row in rows:\n",
    "    # Perform some operations on the row\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164ff96a-b24e-47ae-8cfb-a3be11b990d3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Updating data in pyspark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92135c44-05aa-40bd-a453-53e95b3cc09e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+\n",
      "|   name|age|lessthan30|\n",
      "+-------+---+----------+\n",
      "|  Rahul| 29|         1|\n",
      "|Aashish| 28|        28|\n",
      "|  Aashu| 30|        30|\n",
      "|Sheetal| 22|        22|\n",
      "+-------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "df = spark.createDataFrame([('Rahul', 29), (\"Aashish\", 28), (\"Aashu\", 30), ('Sheetal',22)], ['name', 'age'])\n",
    "\n",
    "df = df.withColumn(\"lessthan30\", when((col(\"name\") == \"Rahul\") & (col(\"age\") < 30), 30-col(\"age\")).otherwise(col(\"age\")))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aec6353f-bb66-4d6a-b398-11f96578c7bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+\n",
      "|lessthan30|   name|\n",
      "+----------+-------+\n",
      "|        30|  Rahul|\n",
      "|        28|Aashish|\n",
      "|        30|  Aashu|\n",
      "|        22|  Bittu|\n",
      "+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.selectExpr(\n",
    "    \"CASE WHEN name = 'Rahul' AND age < 30 THEN 30 ELSE age END AS lessthan30\",\n",
    "    \"CASE WHEN name = 'Sheetal' THEN 'Bittu' ELSE name END AS name\"\n",
    ")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbeaa25-b87a-4878-ba8f-03562bc7c840",
   "metadata": {},
   "source": [
    "### Aggregating Pyspark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74e407fd-5f7e-4e4d-87e6-bf80a51a3c2b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/01 13:55:16 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "+------+--------+\n",
      "|gender|avg(age)|\n",
      "+------+--------+\n",
      "|     F|    35.0|\n",
      "|     M|    35.0|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"Pyspark aggregation\").getOrCreate()\n",
    "\n",
    "# Create a sample DataFrame\n",
    "data = [(\"Alice\", \"F\", 25), (\"Bob\", \"M\", 30), (\"Charlie\", \"M\", 35), (\"Dave\", \"M\", 40), (\"Eve\", \"F\", 45)]\n",
    "df = spark.createDataFrame(data, [\"name\", \"gender\", \"age\"])\n",
    "\n",
    "# Group the DataFrame by gender and compute the average age\n",
    "agg_df = df.groupBy(\"gender\").agg({\"age\": \"avg\"})\n",
    "\n",
    "# Display the aggregated DataFrame\n",
    "agg_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f57fc436-4a91-450e-b1b6-e6b6c23f3358",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|gender|avg(age)|\n",
      "+------+--------+\n",
      "|     F|    35.0|\n",
      "|     M|    35.0|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "agg_df = df.groupBy(\"gender\").agg({\"age\": \"sum\", \"age\": \"avg\"})\n",
    "\n",
    "# Display the aggregated DataFrame\n",
    "agg_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef94a07-50d3-48a9-ab44-f140fab8ebf4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
